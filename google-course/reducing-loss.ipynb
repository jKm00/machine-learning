{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Loss\n",
    "\n",
    "\"Hot and Cold\" kid's game, where you want to find a hidden object. Start with a random guess, and get feedback on how warm or cold you are (loss). Then make another guess, get feedback and sse if you are now warmer og colder (closer/ less loss).\n",
    "\n",
    "## Iterative approach - example\n",
    "\n",
    "The model takes in one feature and returns one prediction:\n",
    "\n",
    "$$y' = b + W_1x_1$$\n",
    "\n",
    "The initial values set for $b$ and $w_1$ arent' important for linear regression. We'll use:\n",
    "\n",
    "- $b = 0$\n",
    "- $w_1 = 0$\n",
    "\n",
    "With the first feature value 10 yields:\n",
    "\n",
    "$$y' = 0 + 0 * 10 = 0$$\n",
    "\n",
    "Then we use a loss function to determine the loss. Suppose we use the squared loss function that takes in two input values:\n",
    "\n",
    "- $y'$: The model's prediction for features x\n",
    "- $y$: the correct label corresponding to features x\n",
    "\n",
    "Now the machine learning system has to examine the values of the loss function and generate new values for $b$ and $w_1$ and the learning continues iterating.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Convex problems have only one minimum, where the loss function converges.\n",
    "\n",
    "If we've had the time to compute the loss for all possibles values of $w_1$, we would get a bowl-shaped function. Calculating the loss function for every value of $w_1$ would be inefficient way. Thats why we use **gradient descent**\n",
    "\n",
    "The gradient descent algorithm calculates the graditnet of the loss curve. The gradient is equal to the derivative of the curve. The gradient is a vector with both:\n",
    "\n",
    "- direction\n",
    "- magnitude\n",
    "\n",
    "Depending on if the decendent is negative or positive we would move closer to the bowl's minimum.\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    "Since the gradient is a vector with both direction and magnitude, the gradient descent algorithms multiply the gradient by a scalar know as the **learning rate**.\n",
    "\n",
    "**Hyperparameters** are the knobs that programmers tweak in machine learning. Learning rate is one of them. Too small, and the learning will take too long. Too long, and the learning will never find the minimum.\n",
    "\n",
    "**Goldilocks:** the perfect learning rate to find the minimum in the fewest steps possible\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "A **batch** is the total number of examples used to calculate the gradient in a single iteration. When the data set becomes huge, with many examples and each example has many features, one iteration can take very long time to compute.\n",
    "\n",
    "A large data set with randomly selected examples probably contains redundant data. The large the batch the more likely for redundancy. Enormous batches tend not to carry much more predictive value than large batches.\n",
    "\n",
    "**Stochastic gradient descent** uses only a single example (batch size of 1) per iteration. SGD works but is very noisy\n",
    "\n",
    "**Mini-batch stichastic graditn descent** is a compromise. Typically with a batch between 10-1000 examples randomly chosen. Less noisy than SGD and more efficient than full-batch.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
